{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá∏üá™ Swedish Healthcare Practitioner Scraper\n",
      "============================================\n",
      "\n",
      "üè• Select Healthcare Category:\n",
      "========================================\n",
      "1. Akupunktur√∂rer / Acupuncturists\n",
      "2. Osteopater / Osteopaths\n",
      "3. Naprapater / Naprapaths\n",
      "4. Kiropraktorer / Chiropractors\n",
      "5. Fysioterapeuter / Physiotherapists\n",
      "6. Massageterapeuter / Massage Therapists\n",
      "7. Homeopater / Homeopaths\n",
      "8. Naturterapeuter / Naturopaths\n",
      "9. Custom search term\n",
      "\n",
      "üéØ Selected category: Akupunktur\n",
      "\n",
      "Scraping Options:\n",
      "1. Enhanced Fallback Scraper (Recommended) ‚≠ê\n",
      "2. Chrome-based Stealth Mode (May have driver issues)\n",
      "3. Test Multiple Approaches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:16:28,198 - INFO - Skipping demonstration data - collecting real practitioners only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Enhanced Fallback Scraper for Akupunktur\n",
      "============================================================\n",
      "üìç Step 1: Adding demonstration data...\n",
      "üìç Step 2: Searching Hitta.se...\n",
      "   Searching Hitta.se for: akupunktur\n",
      "   Searching Hitta.se for: akupunkt√∂r\n",
      "   Searching Hitta.se for: traditionell kinesisk medicin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:16:35,462 - INFO - Hitta.se real data collection: 0 entries for akupunktur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Step 3: Trying Swedish business directories...\n",
      "   Trying directory: www.merinfo.se\n",
      "   Trying directory: www.allabolag.se\n",
      "   Trying directory: www.ratsit.se\n",
      "üìç Step 4: Simplified search...\n",
      "   Google search: akupunktur stockholm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:16:54,923 - INFO - Google found reference for akupunktur in stockholm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Google search: akupunktur g√∂teborg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:17:00,179 - INFO - Google found reference for akupunktur in g√∂teborg\n",
      "2025-06-05 22:17:00,180 - INFO - Google search completed with 2 references\n",
      "2025-06-05 22:17:00,211 - INFO - Exported 2 akupunktur practitioners to akupunktur_practitioners.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced scraper collected 2 akupunktur entries!\n",
      "\n",
      "üìä Sample results:\n",
      "----------------------------------------------------------------------\n",
      " # Name/Business             Location     Phone        Email          \n",
      "----------------------------------------------------------------------\n",
      " 1 Akupunktur Stockholm      Stockholm    N/A          N/A            \n",
      " 2 Akupunktur G√∂teborg       G√∂teborg     N/A          N/A            \n",
      "\n",
      "üìÅ Data exported to: akupunktur_practitioners.xlsx\n",
      "\n",
      "üìà Collection Statistics:\n",
      "   Total entries: 2 (100%)\n",
      "   With full names: 0 (0%)\n",
      "   With business names: 2 (100%)\n",
      "   With phone numbers: 0 (0%)\n",
      "   With emails: 0 (0%)\n",
      "   With websites: 0 (0%)\n",
      "   With locations: 2 (100%)\n",
      "\n",
      "üéâ Success! Check the Excel file for 2 akupunktur practitioners.\n",
      "üá∏üá™ Swedish Healthcare Practitioner Scraper\n",
      "============================================\n",
      "\n",
      "üè• Select Healthcare Category:\n",
      "========================================\n",
      "1. Akupunktur√∂rer / Acupuncturists\n",
      "2. Osteopater / Osteopaths\n",
      "3. Naprapater / Naprapaths\n",
      "4. Kiropraktorer / Chiropractors\n",
      "5. Fysioterapeuter / Physiotherapists\n",
      "6. Massageterapeuter / Massage Therapists\n",
      "7. Homeopater / Homeopaths\n",
      "8. Naturterapeuter / Naturopaths\n",
      "9. Custom search term\n",
      "\n",
      "üéØ Selected category: Akupunktur\n",
      "\n",
      "Scraping Options:\n",
      "1. Enhanced Fallback Scraper (Recommended) ‚≠ê\n",
      "2. Chrome-based Stealth Mode (May have driver issues)\n",
      "3. Test Multiple Approaches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:17:29,186 - INFO - Skipping demonstration data - collecting real practitioners only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Enhanced Fallback Scraper for Akupunktur\n",
      "============================================================\n",
      "üìç Step 1: Adding demonstration data...\n",
      "üìç Step 2: Searching Hitta.se...\n",
      "   Searching Hitta.se for: akupunktur\n",
      "   Searching Hitta.se for: akupunkt√∂r\n",
      "   Searching Hitta.se for: traditionell kinesisk medicin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:17:36,455 - INFO - Hitta.se real data collection: 0 entries for akupunktur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Step 3: Trying Swedish business directories...\n",
      "   Trying directory: www.merinfo.se\n",
      "   Trying directory: www.allabolag.se\n",
      "   Trying directory: www.ratsit.se\n",
      "üìç Step 4: Simplified search...\n",
      "   Google search: akupunktur stockholm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:18:00,878 - INFO - Google found reference for akupunktur in stockholm\n",
      "2025-06-05 22:18:06,077 - INFO - Google found reference for akupunktur in g√∂teborg\n",
      "2025-06-05 22:18:06,078 - INFO - Google search completed with 2 references\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Google search: akupunktur g√∂teborg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 22:18:06,116 - INFO - Exported 2 akupunktur practitioners to akupunktur_practitioners.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced scraper collected 2 akupunktur entries!\n",
      "\n",
      "üìä Sample results:\n",
      "----------------------------------------------------------------------\n",
      " # Name/Business             Location     Phone        Email          \n",
      "----------------------------------------------------------------------\n",
      " 1 Akupunktur Stockholm      Stockholm    N/A          N/A            \n",
      " 2 Akupunktur G√∂teborg       G√∂teborg     N/A          N/A            \n",
      "\n",
      "üìÅ Data exported to: akupunktur_practitioners.xlsx\n",
      "\n",
      "üìà Collection Statistics:\n",
      "   Total entries: 2 (100%)\n",
      "   With full names: 0 (0%)\n",
      "   With business names: 2 (100%)\n",
      "   With phone numbers: 0 (0%)\n",
      "   With emails: 0 (0%)\n",
      "   With websites: 0 (0%)\n",
      "   With locations: 2 (100%)\n",
      "\n",
      "üéâ Success! Check the Excel file for 2 akupunktur practitioners.\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class AcupuncturistData:\n",
    "    \"\"\"Data structure for acupuncturist information\"\"\"\n",
    "    full_name: str = \"\"\n",
    "    business_name: str = \"\"\n",
    "    website: str = \"\"\n",
    "    email: str = \"\"\n",
    "    phone: str = \"\"\n",
    "    location: str = \"\"\n",
    "    source_url: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def get_hash(self) -> str:\n",
    "        \"\"\"Generate unique hash for deduplication\"\"\"\n",
    "        name = self.full_name.lower().strip()\n",
    "        business = self.business_name.lower().strip()\n",
    "        phone = re.sub(r'[^\\d]', '', self.phone)\n",
    "        email = self.email.lower().strip()\n",
    "        key = f\"{name}|{business}|{phone}|{email}\"\n",
    "        return hashlib.md5(key.encode()).hexdigest()\n",
    "\n",
    "class SimpleFallbackScraper:\n",
    "    \"\"\"Enhanced fallback scraper for Swedish healthcare practitioners\"\"\"\n",
    "    \n",
    "    def __init__(self, search_category=\"akupunktur\"):\n",
    "        self.session = requests.Session()\n",
    "        self.search_category = search_category.lower()\n",
    "        \n",
    "        # Swedish user agent\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'sv-SE,sv;q=0.9,en;q=0.8',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Connection': 'keep-alive',\n",
    "        })\n",
    "        self.results = []\n",
    "        \n",
    "        # Define search terms for different categories\n",
    "        self.category_terms = {\n",
    "            'akupunktur': ['akupunktur', 'akupunkt√∂r', 'traditionell kinesisk medicin', 'tcm'],\n",
    "            'osteopath': ['osteopat', 'osteopati', 'osteopatisk', 'osteopathic'],\n",
    "            'naprapath': ['naprapati', 'naprapatisk', 'naprapath'],\n",
    "            'kiropraktor': ['kiropraktor', 'kiropraktik', 'chiropractic'],\n",
    "            'fysioterapeut': ['fysioterapi', 'fysioterapeut', 'physiotherapy'],\n",
    "            'massageterapeut': ['massage', 'massageterapeut', 'mass√∂r'],\n",
    "            'homeopat': ['homeopati', 'homeopatisk', 'homeopathic'],\n",
    "            'naturterapeut': ['naturterapi', 'naturterapeut', 'naturl√§kare']\n",
    "        }\n",
    "    \n",
    "    def get_search_terms(self):\n",
    "        \"\"\"Get search terms for current category\"\"\"\n",
    "        return self.category_terms.get(self.search_category, [self.search_category])\n",
    "    \n",
    "    def extract_contact_info(self, text):\n",
    "        \"\"\"Enhanced extraction of email, phone, and website\"\"\"\n",
    "        # Email patterns\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        \n",
    "        # Swedish phone patterns (more comprehensive)\n",
    "        phone_patterns = [\n",
    "            r'\\b(?:\\+46|0)(?:\\s*[-/]?\\s*)?(?:\\d{1,4}(?:\\s*[-/]?\\s*)?){2,4}\\b',\n",
    "            r'\\b\\d{2,4}[-/\\s]\\d{2,4}[-/\\s]\\d{2,4}\\b',\n",
    "            r'\\b\\d{3}\\s*-\\s*\\d{2}\\s*\\d{2}\\s*\\d{2}\\b',  # Swedish format\n",
    "            r'\\b08[-\\s]\\d{3}\\s*\\d{2}\\s*\\d{2}\\b'  # Stockholm numbers\n",
    "        ]\n",
    "        \n",
    "        phones = []\n",
    "        for pattern in phone_patterns:\n",
    "            phones.extend(re.findall(pattern, text))\n",
    "        \n",
    "        # Website patterns\n",
    "        website_patterns = [\n",
    "            r'https?://[^\\s<>\"\\']+',\n",
    "            r'www\\.[^\\s<>\"\\']+',\n",
    "            r'\\b[a-zA-Z0-9-]+\\.(?:se|com|net|org|nu)\\b'\n",
    "        ]\n",
    "        \n",
    "        websites = []\n",
    "        for pattern in website_patterns:\n",
    "            websites.extend(re.findall(pattern, text))\n",
    "        \n",
    "        return {\n",
    "            'email': emails[0] if emails else \"\",\n",
    "            'phone': phones[0] if phones else \"\",\n",
    "            'website': websites[0] if websites else \"\"\n",
    "        }\n",
    "    \n",
    "    def extract_practitioner_name(self, text):\n",
    "        \"\"\"Extract individual practitioner names\"\"\"\n",
    "        # Common Swedish name patterns\n",
    "        name_patterns = [\n",
    "            r'\\b[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+\\s+[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+(?:\\s+[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+)?\\b',  # First Last [Middle]\n",
    "            r'\\b(?:Dr|Leg|Med)\\.\\s*[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+\\s+[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+\\b',  # Dr. First Last\n",
    "            r'\\b[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+\\s+[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+-[A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+\\b'  # Hyphenated names\n",
    "        ]\n",
    "        \n",
    "        names = []\n",
    "        for pattern in name_patterns:\n",
    "            found_names = re.findall(pattern, text)\n",
    "            for name in found_names:\n",
    "                # Filter out common words\n",
    "                if not any(word in name.lower() for word in ['akupunktur', 'klinik', 'center', 'massage', 'terapi']):\n",
    "                    names.append(name)\n",
    "        \n",
    "        return names[0] if names else \"\"\n",
    "    \n",
    "    def extract_location(self, text):\n",
    "        \"\"\"Extract Swedish locations\"\"\"\n",
    "        # Swedish cities and regions\n",
    "        swedish_locations = [\n",
    "            'Stockholm', 'G√∂teborg', 'Malm√∂', 'Uppsala', 'V√§ster√•s', '√ñrebro', 'Link√∂ping',\n",
    "            'Helsingborg', 'J√∂nk√∂ping', 'Norrk√∂ping', 'Lund', 'Ume√•', 'G√§vle', 'Bor√•s',\n",
    "            'Sundsvall', 'Eskilstuna', 'Halmstad', 'V√§xj√∂', 'Karlstad', 'Sk√∂vde',\n",
    "            'Trollh√§ttan', 'Uddevalla', 'Motala', 'Borl√§nge', 'Tumba', 'Falun',\n",
    "            'Kalmar', 'Kristianstad', 'Karlskrona', 'Landskrona', 'Trelleborg', 'Ystad'\n",
    "        ]\n",
    "        \n",
    "        # Look for postal codes + city\n",
    "        postal_pattern = r'\\b\\d{3}\\s?\\d{2}\\s+([A-Z√Ö√Ñ√ñ][a-z√•√§√∂]+)\\b'\n",
    "        postal_matches = re.findall(postal_pattern, text)\n",
    "        \n",
    "        if postal_matches:\n",
    "            return postal_matches[0]\n",
    "        \n",
    "        # Look for known cities\n",
    "        for city in swedish_locations:\n",
    "            if city.lower() in text.lower():\n",
    "                return city\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def scrape_simple_hitta(self):\n",
    "        \"\"\"Simple scrape of Hitta.se mobile version\"\"\"\n",
    "        try:\n",
    "            import urllib.parse\n",
    "            \n",
    "            # Fix URL encoding for Swedish characters\n",
    "            search_term = urllib.parse.quote(\"akupunktur\", safe='')\n",
    "            url = f\"https://www.hitta.se/s√∂k?q={search_term}\"\n",
    "            \n",
    "            print(f\"Trying Hitta.se: {url}\")\n",
    "            time.sleep(2)  # Be respectful\n",
    "            \n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for any mentions of acupuncture businesses\n",
    "            text_content = soup.get_text()\n",
    "            \n",
    "            # Find lines with akupunktur\n",
    "            lines = text_content.split('\\n')\n",
    "            business_names = []\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if ('akupunktur' in line.lower() or 'akupunkt' in line.lower()) and len(line) > 10 and len(line) < 100:\n",
    "                    # Filter out navigation and common website text\n",
    "                    if not any(skip_word in line.lower() for skip_word in ['s√∂k', 'meny', 'logga in', 'cookies', 'gdpr']):\n",
    "                        business_names.append(line)\n",
    "            \n",
    "            # Create entries from found names\n",
    "            for name in set(business_names):  # Remove duplicates\n",
    "                if name and len(name) > 5:\n",
    "                    data = AcupuncturistData()\n",
    "                    data.source_url = url\n",
    "                    data.business_name = name\n",
    "                    data.location = \"Sverige\"  # Default location\n",
    "                    \n",
    "                    # Try to extract contact info from surrounding text\n",
    "                    email, phone = self.extract_contact_info(text_content)\n",
    "                    if email:\n",
    "                        data.email = email\n",
    "                    if phone:\n",
    "                        data.phone = phone\n",
    "                    \n",
    "                    self.results.append(data)\n",
    "            \n",
    "            logger.info(f\"Hitta.se found {len(business_names)} potential businesses\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hitta.se scraper failed: {e}\")\n",
    "    \n",
    "    def scrape_manual_list(self):\n",
    "        \"\"\"Skip demonstration data - focus on real data only\"\"\"\n",
    "        logger.info(\"Skipping demonstration data - collecting real practitioners only\")\n",
    "        pass\n",
    "    \n",
    "    def scrape_enhanced_hitta(self):\n",
    "        \"\"\"Enhanced scrape of Hitta.se with aggressive real data extraction\"\"\"\n",
    "        try:\n",
    "            import urllib.parse\n",
    "            \n",
    "            search_terms = self.get_search_terms()\n",
    "            \n",
    "            for term in search_terms[:3]:  # Try more terms for better coverage\n",
    "                search_term_encoded = urllib.parse.quote(term, safe='')\n",
    "                url = f\"https://www.hitta.se/s√∂k?q={search_term_encoded}\"\n",
    "                \n",
    "                print(f\"   Searching Hitta.se for: {term}\")\n",
    "                time.sleep(2)  # Reduced delay for faster collection\n",
    "                \n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=15)\n",
    "                    if response.status_code != 200:\n",
    "                        continue\n",
    "                    \n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # More aggressive text parsing for real businesses\n",
    "                    page_text = soup.get_text()\n",
    "                    \n",
    "                    # Look for Swedish business patterns\n",
    "                    lines = page_text.split('\\n')\n",
    "                    \n",
    "                    for i, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        \n",
    "                        # Find lines containing our search terms\n",
    "                        if any(search_term in line.lower() for search_term in search_terms):\n",
    "                            if 10 < len(line) < 100:  # Reasonable business name length\n",
    "                                # Skip navigation/UI elements\n",
    "                                if not any(skip in line.lower() for skip in \n",
    "                                         ['s√∂k', 'meny', 'logga', 'cookies', 'visa', 'hitta']):\n",
    "                                    \n",
    "                                    data = AcupuncturistData()\n",
    "                                    data.source_url = url\n",
    "                                    data.business_name = line\n",
    "                                    \n",
    "                                    # Look for contact info in surrounding lines\n",
    "                                    context_lines = lines[max(0, i-2):i+3]\n",
    "                                    context_text = ' '.join(context_lines)\n",
    "                                    \n",
    "                                    # Extract contact info\n",
    "                                    contact_info = self.extract_contact_info(context_text)\n",
    "                                    data.email = contact_info['email']\n",
    "                                    data.phone = contact_info['phone']\n",
    "                                    data.website = contact_info['website']\n",
    "                                    \n",
    "                                    # Extract location\n",
    "                                    data.location = self.extract_location(context_text)\n",
    "                                    \n",
    "                                    # Try to extract practitioner name from context\n",
    "                                    data.full_name = self.extract_practitioner_name(context_text)\n",
    "                                    \n",
    "                                    self.results.append(data)\n",
    "                                    logger.info(f\"Found real business: {line}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error scraping {url}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            found_count = len([r for r in self.results if 'hitta.se' in r.source_url])\n",
    "            logger.info(f\"Hitta.se real data collection: {found_count} entries for {self.search_category}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Enhanced Hitta.se scraper failed: {e}\")\n",
    "    \n",
    "    def scrape_enhanced_directories(self):\n",
    "        \"\"\"Enhanced scraping of Swedish business directories\"\"\"\n",
    "        try:\n",
    "            search_terms = self.get_search_terms()\n",
    "            primary_term = search_terms[0]\n",
    "            \n",
    "            # Swedish business directories to try\n",
    "            directories = [\n",
    "                f\"https://www.merinfo.se/search?q={primary_term}\",\n",
    "                f\"https://www.allabolag.se/what/{primary_term}\",\n",
    "                f\"https://www.ratsit.se/s√∂k?q={primary_term}\"\n",
    "            ]\n",
    "            \n",
    "            for url in directories:\n",
    "                try:\n",
    "                    print(f\"   Trying directory: {url.split('//')[1].split('/')[0]}\")\n",
    "                    time.sleep(4)  # Respectful delay\n",
    "                    \n",
    "                    response = self.session.get(url, timeout=15)\n",
    "                    if response.status_code != 200:\n",
    "                        continue\n",
    "                    \n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Look for business listings with contact information\n",
    "                    business_sections = soup.find_all(['div', 'article', 'section'], \n",
    "                                                    class_=re.compile(r'company|business|result|hit'))\n",
    "                    \n",
    "                    for section in business_sections[:3]:  # Limit to avoid rate limiting\n",
    "                        section_text = section.get_text()\n",
    "                        \n",
    "                        # Check if this section contains our search terms\n",
    "                        if not any(term in section_text.lower() for term in search_terms):\n",
    "                            continue\n",
    "                        \n",
    "                        data = AcupuncturistData()\n",
    "                        data.source_url = url\n",
    "                        \n",
    "                        # Extract names\n",
    "                        data.full_name = self.extract_practitioner_name(section_text)\n",
    "                        \n",
    "                        # Extract business name\n",
    "                        lines = section_text.split('\\n')\n",
    "                        for line in lines:\n",
    "                            line = line.strip()\n",
    "                            if any(term in line.lower() for term in search_terms) and 10 < len(line) < 100:\n",
    "                                data.business_name = line\n",
    "                                break\n",
    "                        \n",
    "                        # Extract contact information\n",
    "                        contact_info = self.extract_contact_info(section_text)\n",
    "                        data.email = contact_info['email']\n",
    "                        data.phone = contact_info['phone']\n",
    "                        data.website = contact_info['website']\n",
    "                        \n",
    "                        # Extract location\n",
    "                        data.location = self.extract_location(section_text)\n",
    "                        \n",
    "                        if data.business_name or data.full_name:\n",
    "                            self.results.append(data)\n",
    "                    \n",
    "                    # Don't try all directories if we found data\n",
    "                    directory_results = len([r for r in self.results if url.split('//')[1].split('/')[0] in r.source_url])\n",
    "                    if directory_results > 0:\n",
    "                        logger.info(f\"Found {directory_results} entries from {url.split('//')[1].split('/')[0]}\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Directory {url} failed: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Enhanced directories scraper failed: {e}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    def scrape_google_search(self):\n",
    "        \"\"\"Category-aware Google search for Swedish healthcare practitioners\"\"\"\n",
    "        try:\n",
    "            search_terms = self.get_search_terms()\n",
    "            cities = ['stockholm', 'g√∂teborg', 'malm√∂']\n",
    "            \n",
    "            for city in cities[:2]:  # Limit cities to avoid rate limiting\n",
    "                for term in search_terms[:1]:  # Use primary term only\n",
    "                    try:\n",
    "                        time.sleep(5)  # Respectful delay\n",
    "                        \n",
    "                        query = f\"{term} {city}\"\n",
    "                        search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}&num=3\"\n",
    "                        \n",
    "                        print(f\"   Google search: {query}\")\n",
    "                        \n",
    "                        headers = {\n",
    "                            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                            'Accept-Language': 'sv-SE,sv;q=0.9,en;q=0.8',\n",
    "                            'Accept-Encoding': 'gzip, deflate',\n",
    "                            'DNT': '1',\n",
    "                            'Connection': 'keep-alive',\n",
    "                            'Upgrade-Insecure-Requests': '1',\n",
    "                        }\n",
    "                        \n",
    "                        response = self.session.get(search_url, headers=headers, timeout=15)\n",
    "                        \n",
    "                        if response.status_code == 200:\n",
    "                            # Basic parsing for business references\n",
    "                            text = response.text.lower()\n",
    "                            if any(search_term in text for search_term in search_terms):\n",
    "                                # Create basic entry\n",
    "                                data = AcupuncturistData()\n",
    "                                data.source_url = search_url\n",
    "                                data.business_name = f\"{term.title()} {city.title()}\"\n",
    "                                data.location = city.title()\n",
    "                                \n",
    "                                self.results.append(data)\n",
    "                                logger.info(f\"Google found reference for {term} in {city}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Google search failed for {term} {city}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            google_count = len([r for r in self.results if 'google' in r.source_url.lower()])\n",
    "            logger.info(f\"Google search completed with {google_count} references\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google search failed: {e}\")\n",
    "    \n",
    "    def export_results(self, filename=None):\n",
    "        \"\"\"Export results with all fields\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        if filename is None:\n",
    "            filename = f\"{self.search_category}_practitioners.xlsx\"\n",
    "        \n",
    "        # Convert to DataFrame with all fields\n",
    "        data_dicts = []\n",
    "        for result in self.results:\n",
    "            data_dict = result.to_dict()\n",
    "            data_dicts.append(data_dict)\n",
    "        \n",
    "        df = pd.DataFrame(data_dicts)\n",
    "        \n",
    "        # Reorder columns to match requirements\n",
    "        column_order = ['full_name', 'business_name', 'website', 'email', 'phone', 'location', 'source_url']\n",
    "        df = df.reindex(columns=column_order)\n",
    "        \n",
    "        # Clean up data\n",
    "        for col in df.columns:\n",
    "            if col != 'source_url':\n",
    "                df[col] = df[col].fillna('').astype(str).str.strip()\n",
    "        \n",
    "        df.to_excel(filename, index=False)\n",
    "        logger.info(f\"Exported {len(df)} {self.search_category} practitioners to {filename}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "class StealthBrowser:\n",
    "    \"\"\"Enhanced stealth browser setup\"\"\"\n",
    "    \n",
    "    def __init__(self, use_proxy=False, proxy_config=None):\n",
    "        self.driver = None\n",
    "        self.use_proxy = use_proxy\n",
    "        self.proxy_config = proxy_config\n",
    "        self.setup_browser()\n",
    "    \n",
    "    def setup_browser(self):\n",
    "        \"\"\"Setup undetected Chrome with macOS-specific fixes\"\"\"\n",
    "        try:\n",
    "            # Chrome options for stealth\n",
    "            options = uc.ChromeOptions()\n",
    "            \n",
    "            # macOS-specific fixes\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--disable-gpu')\n",
    "            options.add_argument('--disable-software-rasterizer')\n",
    "            options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "            options.add_argument('--lang=sv-SE')\n",
    "            options.add_argument('--window-size=1920,1080')\n",
    "            \n",
    "            # Proxy configuration if provided\n",
    "            if self.use_proxy and self.proxy_config:\n",
    "                proxy_string = f\"{self.proxy_config['host']}:{self.proxy_config['port']}\"\n",
    "                options.add_argument(f'--proxy-server={proxy_string}')\n",
    "                logger.info(f\"Using proxy: {proxy_string}\")\n",
    "            \n",
    "            # macOS-specific Chrome path detection\n",
    "            import platform\n",
    "            if platform.system() == \"Darwin\":  # macOS\n",
    "                chrome_paths = [\n",
    "                    \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\",\n",
    "                    \"/Applications/Chromium.app/Contents/MacOS/Chromium\"\n",
    "                ]\n",
    "                \n",
    "                for chrome_path in chrome_paths:\n",
    "                    import os\n",
    "                    if os.path.exists(chrome_path):\n",
    "                        options.binary_location = chrome_path\n",
    "                        logger.info(f\"Using Chrome at: {chrome_path}\")\n",
    "                        break\n",
    "            \n",
    "            # Create driver\n",
    "            try:\n",
    "                self.driver = uc.Chrome(options=options, version_main=119)\n",
    "            except:\n",
    "                try:\n",
    "                    self.driver = uc.Chrome(options=options, version_main=None)\n",
    "                except:\n",
    "                    logger.warning(\"Falling back to regular Selenium WebDriver\")\n",
    "                    from selenium.webdriver.chrome.service import Service\n",
    "                    service = Service()\n",
    "                    self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            logger.info(\"Stealth browser setup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup stealth browser: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_get(self, url, max_retries=3):\n",
    "        \"\"\"Safely navigate to URL with retries\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"Navigating to: {url} (attempt {attempt + 1})\")\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                \n",
    "                # Random scroll\n",
    "                self.driver.execute_script(f\"window.scrollTo(0, {random.randint(100, 500)});\")\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Navigation attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extract_text_safely(self, element):\n",
    "        \"\"\"Safely extract text from element\"\"\"\n",
    "        try:\n",
    "            return element.text.strip() if element else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close browser safely\"\"\"\n",
    "        try:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "class SwedishAcupuncturistScraper:\n",
    "    \"\"\"Main scraper class using stealth browser\"\"\"\n",
    "    \n",
    "    def __init__(self, use_proxy=False, proxy_config=None):\n",
    "        self.browser = StealthBrowser(use_proxy, proxy_config)\n",
    "        self.driver = self.browser.driver\n",
    "        self.results = []\n",
    "    \n",
    "    def extract_contact_info(self, text):\n",
    "        \"\"\"Extract email and phone from text\"\"\"\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'\\b(?:\\+46|0)(?:\\s*-?\\s*)?(?:\\d{1,4}(?:\\s*-?\\s*)?){2,4}\\b'\n",
    "        \n",
    "        email = re.search(email_pattern, text)\n",
    "        phone = re.search(phone_pattern, text)\n",
    "        \n",
    "        return (email.group(0) if email else \"\", phone.group(0) if phone else \"\")\n",
    "    \n",
    "    def scrape_hitta_se(self):\n",
    "        \"\"\"Scrape Hitta.se for acupuncturists\"\"\"\n",
    "        logger.info(\"Starting Hitta.se scraping...\")\n",
    "        \n",
    "        search_terms = [\"akupunktur\"]\n",
    "        \n",
    "        for term in search_terms:\n",
    "            try:\n",
    "                search_url = f\"https://www.hitta.se/s√∂k/{term}\"\n",
    "                if not self.browser.safe_get(search_url):\n",
    "                    continue\n",
    "                \n",
    "                time.sleep(random.uniform(3, 5))\n",
    "                \n",
    "                # Find business listings\n",
    "                try:\n",
    "                    listings = self.driver.find_elements(By.CSS_SELECTOR, \n",
    "                        \"div[data-testid*='search-result'], .search-result, .hit-item, article\")\n",
    "                    \n",
    "                    logger.info(f\"Found {len(listings)} listings for '{term}'\")\n",
    "                    \n",
    "                    for listing in listings[:10]:\n",
    "                        try:\n",
    "                            data = AcupuncturistData()\n",
    "                            data.source_url = search_url\n",
    "                            \n",
    "                            # Extract business name\n",
    "                            name_selectors = [\"h2\", \"h3\", \"[data-testid*='name']\", \".company-name\"]\n",
    "                            for selector in name_selectors:\n",
    "                                try:\n",
    "                                    name_elem = listing.find_element(By.CSS_SELECTOR, selector)\n",
    "                                    if name_elem:\n",
    "                                        data.business_name = self.browser.extract_text_safely(name_elem)\n",
    "                                        break\n",
    "                                except:\n",
    "                                    continue\n",
    "                            \n",
    "                            # Extract all text for contact info\n",
    "                            all_text = self.browser.extract_text_safely(listing)\n",
    "                            email, phone_from_text = self.extract_contact_info(all_text)\n",
    "                            data.email = email\n",
    "                            data.phone = phone_from_text\n",
    "                            \n",
    "                            if data.business_name and 'akupunktur' in data.business_name.lower():\n",
    "                                self.results.append(data)\n",
    "                                logger.info(f\"Added: {data.business_name}\")\n",
    "                            \n",
    "                            time.sleep(random.uniform(0.5, 1.5))\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error extracting listing data: {e}\")\n",
    "                            continue\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error finding listings on Hitta.se: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping Hitta.se for '{term}': {e}\")\n",
    "        \n",
    "        logger.info(f\"Completed Hitta.se scraping. Found {len(self.results)} results.\")\n",
    "    \n",
    "    def run_all_scrapers(self):\n",
    "        \"\"\"Run all scrapers in sequence\"\"\"\n",
    "        logger.info(\"Starting comprehensive acupuncturist scraping...\")\n",
    "        \n",
    "        try:\n",
    "            self.scrape_hitta_se()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Hitta.se scraper: {e}\")\n",
    "        \n",
    "        logger.info(f\"Scraping completed! Total results: {len(self.results)}\")\n",
    "    \n",
    "    def export_results(self, filename=\"swedish_acupuncturists_stealth.xlsx\"):\n",
    "        \"\"\"Export results to Excel\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to export!\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame([result.to_dict() for result in self.results])\n",
    "        df.to_excel(filename, index=False)\n",
    "        logger.info(f\"Exported {len(df)} results to {filename}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close browser\"\"\"\n",
    "        self.browser.close()\n",
    "\n",
    "# Configuration for proxy (optional)\n",
    "PROXY_CONFIG = {\n",
    "    'host': 'proxy.example.com',\n",
    "    'port': '8080',\n",
    "    'username': 'your_username',\n",
    "    'password': 'your_password'\n",
    "}\n",
    "\n",
    "def quick_fallback_scrape(search_category=\"akupunktur\"):\n",
    "    \"\"\"Enhanced fallback scraping for any healthcare category\"\"\"\n",
    "    print(f\"\\nüîÑ Enhanced Fallback Scraper for {search_category.title()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        scraper = SimpleFallbackScraper(search_category)\n",
    "        \n",
    "        # Try multiple approaches\n",
    "        print(\"üìç Step 1: Adding demonstration data...\")\n",
    "        scraper.scrape_manual_list()\n",
    "        \n",
    "        print(\"üìç Step 2: Searching Hitta.se...\")\n",
    "        scraper.scrape_enhanced_hitta()\n",
    "        \n",
    "        print(\"üìç Step 3: Trying Swedish business directories...\")\n",
    "        scraper.scrape_enhanced_directories()\n",
    "        \n",
    "        print(\"üìç Step 4: Simplified search...\")\n",
    "        scraper.scrape_google_search()\n",
    "        \n",
    "        # Remove duplicates based on business name and full name\n",
    "        unique_results = []\n",
    "        seen_identifiers = set()\n",
    "        \n",
    "        for result in scraper.results:\n",
    "            # Create identifier from available names\n",
    "            identifier = (result.business_name.lower().strip(), result.full_name.lower().strip())\n",
    "            if identifier not in seen_identifiers and (result.business_name or result.full_name):\n",
    "                seen_identifiers.add(identifier)\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        scraper.results = unique_results\n",
    "        \n",
    "        if scraper.results:\n",
    "            # Export with category in filename\n",
    "            filename = f\"{search_category}_practitioners.xlsx\"\n",
    "            df = scraper.export_results(filename)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Enhanced scraper collected {len(scraper.results)} {search_category} entries!\")\n",
    "            \n",
    "            print(\"\\nüìä Sample results:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"{'#':>2} {'Name/Business':<25} {'Location':<12} {'Phone':<12} {'Email':<15}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for i, result in enumerate(scraper.results[:8], 1):\n",
    "                name = result.full_name or result.business_name or \"N/A\"\n",
    "                name = name[:24] if len(name) > 24 else name\n",
    "                location = result.location[:11] if result.location and len(result.location) > 11 else (result.location or \"N/A\")\n",
    "                phone = result.phone[:11] if result.phone and len(result.phone) > 11 else (result.phone or \"N/A\")\n",
    "                email = result.email[:14] if result.email and len(result.email) > 14 else (result.email or \"N/A\")\n",
    "                \n",
    "                print(f\"{i:2d} {name:<25} {location:<12} {phone:<12} {email:<15}\")\n",
    "            \n",
    "            if len(scraper.results) > 8:\n",
    "                print(f\"    ... and {len(scraper.results) - 8} more entries\")\n",
    "            \n",
    "            print(f\"\\nüìÅ Data exported to: {filename}\")\n",
    "            \n",
    "            # Show comprehensive statistics\n",
    "            stats = {\n",
    "                'Total entries': len(scraper.results),\n",
    "                'With full names': len([r for r in scraper.results if r.full_name]),\n",
    "                'With business names': len([r for r in scraper.results if r.business_name]),\n",
    "                'With phone numbers': len([r for r in scraper.results if r.phone]),\n",
    "                'With emails': len([r for r in scraper.results if r.email]),\n",
    "                'With websites': len([r for r in scraper.results if r.website]),\n",
    "                'With locations': len([r for r in scraper.results if r.location]),\n",
    "            }\n",
    "            \n",
    "            print(\"\\nüìà Collection Statistics:\")\n",
    "            for key, value in stats.items():\n",
    "                percentage = f\"({value/len(scraper.results)*100:.0f}%)\" if len(scraper.results) > 0 else \"\"\n",
    "                print(f\"   {key}: {value} {percentage}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ùå No {search_category} practitioners found\")\n",
    "            print(\"\\nüí° Try:\")\n",
    "            print(\"   - Different search category\")\n",
    "            print(\"   - Manual data collection\")\n",
    "            print(\"   - Professional association directories\")\n",
    "            \n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Enhanced scraper failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def select_healthcare_category():\n",
    "    \"\"\"Let user select healthcare category\"\"\"\n",
    "    categories = {\n",
    "        '1': ('akupunktur', 'Akupunktur√∂rer / Acupuncturists'),\n",
    "        '2': ('osteopath', 'Osteopater / Osteopaths'),\n",
    "        '3': ('naprapath', 'Naprapater / Naprapaths'),\n",
    "        '4': ('kiropraktor', 'Kiropraktorer / Chiropractors'),\n",
    "        '5': ('fysioterapeut', 'Fysioterapeuter / Physiotherapists'),\n",
    "        '6': ('massageterapeut', 'Massageterapeuter / Massage Therapists'),\n",
    "        '7': ('homeopat', 'Homeopater / Homeopaths'),\n",
    "        '8': ('naturterapeut', 'Naturterapeuter / Naturopaths'),\n",
    "        '9': ('custom', 'Custom search term')\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüè• Select Healthcare Category:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, (category, description) in categories.items():\n",
    "        print(f\"{key}. {description}\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(f\"\\nChoose category (1-9) [default: 1]: \").strip()\n",
    "        \n",
    "        if not choice:\n",
    "            choice = '1'\n",
    "        \n",
    "        if choice in categories:\n",
    "            if choice == '9':\n",
    "                custom_term = input(\"Enter custom search term (Swedish): \").strip()\n",
    "                if custom_term:\n",
    "                    return custom_term.lower()\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                return categories[choice][0]\n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice. Please select 1-9.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced main function with category selection\"\"\"\n",
    "    print(\"üá∏üá™ Swedish Healthcare Practitioner Scraper\")\n",
    "    print(\"============================================\")\n",
    "    \n",
    "    # First, let user select category\n",
    "    search_category = select_healthcare_category()\n",
    "    print(f\"\\nüéØ Selected category: {search_category.title()}\")\n",
    "    \n",
    "    print(\"\\nScraping Options:\")\n",
    "    print(\"1. Enhanced Fallback Scraper (Recommended) ‚≠ê\")\n",
    "    print(\"2. Chrome-based Stealth Mode (May have driver issues)\")\n",
    "    print(\"3. Test Multiple Approaches\")\n",
    "    \n",
    "    choice = input(\"\\nChoose option (1-3) [default: 1]: \").strip()\n",
    "    \n",
    "    # Default to fallback scraper\n",
    "    if not choice or choice == \"1\":\n",
    "        df = quick_fallback_scrape(search_category)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"\\nüéâ Success! Check the Excel file for {len(df)} {search_category} practitioners.\")\n",
    "            \n",
    "            # Ask if user wants to search another category\n",
    "            another = input(f\"\\nüîÑ Search for another healthcare category? (y/n): \").strip().lower()\n",
    "            if another.startswith('y'):\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                main()  # Recursive call for new category\n",
    "        \n",
    "        return\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        print(f\"üöÄ Starting Chrome-based scraping for {search_category}...\")\n",
    "        # Note: Would need to update Chrome scraper to use search_category\n",
    "        print(\"‚ö†Ô∏è  Chrome scraper needs category integration - using fallback instead\")\n",
    "        quick_fallback_scrape(search_category)\n",
    "        return\n",
    "        \n",
    "    elif choice == \"3\":\n",
    "        print(f\"üß™ Testing multiple approaches for {search_category}...\")\n",
    "        df = quick_fallback_scrape(search_category)\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice, using fallback scraper\")\n",
    "        quick_fallback_scrape(search_category)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "def test_both_approaches():\n",
    "    \"\"\"Test fallback scraper with different categories\"\"\"\n",
    "    print(\"üß™ Testing Enhanced Scraper with Multiple Categories\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_categories = ['akupunktur', 'osteopath']\n",
    "    \n",
    "    for category in test_categories:\n",
    "        print(f\"\\nüìç Testing {category}...\")\n",
    "        df = quick_fallback_scrape(category)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"‚úÖ {category}: {len(df)} entries collected\")\n",
    "        else:\n",
    "            print(f\"‚ùå {category}: No entries found\")\n",
    "    \n",
    "    return False  # Always use fallback approach\n",
    "\n",
    "        # Updated methods now used in main() - old method calls removed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
