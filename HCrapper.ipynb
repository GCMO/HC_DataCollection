{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish Acupuncturist Data Scraper\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 17:23:58,472 - INFO - Starting data collection from all sources...\n",
      "2025-06-05 17:23:58,473 - INFO - Running PublicDirectoryScraper...\n",
      "2025-06-05 17:24:10,577 - ERROR - Error fetching https://www.gulasidorna.se/sok?q=akupunktur: Exceeded 30 redirects.\n",
      "2025-06-05 17:24:10,578 - INFO - PublicDirectoryScraper collected 0 entries\n",
      "2025-06-05 17:24:12,582 - INFO - Running GoogleSearchScraper...\n",
      "2025-06-05 17:24:30,784 - INFO - GoogleSearchScraper collected 0 entries\n",
      "2025-06-05 17:24:32,789 - INFO - Running HittaScraper...\n",
      "2025-06-05 17:24:35,512 - ERROR - Error fetching https://www.hitta.se/sök/akupunktur: 404 Client Error: Not Found for url: https://www.hitta.se/s%C3%B6k/akupunktur\n",
      "2025-06-05 17:24:38,393 - ERROR - Error fetching https://www.hitta.se/sök/akupunktör: 404 Client Error: Not Found for url: https://www.hitta.se/s%C3%B6k/akupunkt%C3%B6r\n",
      "2025-06-05 17:24:42,313 - ERROR - Error fetching https://www.hitta.se/sök/traditionell kinesisk medicin: 404 Client Error: Not Found for url: https://www.hitta.se/s%C3%B6k/traditionell%20kinesisk%20medicin\n",
      "2025-06-05 17:24:47,052 - ERROR - Error fetching https://www.hitta.se/sök/TCM: 404 Client Error: Not Found for url: https://www.hitta.se/s%C3%B6k/TCM\n",
      "2025-06-05 17:24:47,052 - INFO - HittaScraper collected 0 entries\n",
      "2025-06-05 17:24:49,056 - INFO - Running RatsitScraper...\n",
      "2025-06-05 17:24:53,554 - WARNING - 403 Forbidden for https://www.ratsit.se/sök?q=akupunktur - trying alternative approach\n",
      "2025-06-05 17:24:53,555 - INFO - RatsitScraper collected 0 entries\n",
      "2025-06-05 17:24:55,556 - INFO - Running HealthProfessionalsScraper...\n",
      "2025-06-05 17:25:04,612 - INFO - HealthProfessionalsScraper collected 0 entries\n",
      "2025-06-05 17:25:06,614 - INFO - Running YellowPagesScraper...\n",
      "2025-06-05 17:25:06,616 - ERROR - Error in YellowPagesScraper: \n",
      "2025-06-05 17:25:06,618 - INFO - Removed 0 duplicates\n",
      "2025-06-05 17:25:06,638 - INFO - Exported 0 unique entries to acupuncturists_sweden.xlsx\n",
      "2025-06-05 17:25:06,639 - INFO - Removed 0 duplicates\n",
      "2025-06-05 17:25:06,643 - INFO - Exported 0 unique entries to acupuncturists_sweden.csv\n",
      "2025-06-05 17:25:06,647 - INFO - Removed 0 duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection Summary:\n",
      "==================\n",
      "Total Collected: 0\n",
      "Unique Entries: 0\n",
      "Duplicates Removed: 0\n",
      "Entries With Email: 0\n",
      "Entries With Phone: 0\n",
      "Entries With Website: 0\n",
      "Business Names: 0\n",
      "Full Names: 0\n",
      "\n",
      "No data collected. Please check the scrapers and try again.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Set\n",
    "import logging\n",
    "import hashlib\n",
    "import random\n",
    "# Removed fake_useragent due to Python 3.8 compatibility issue\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class AcupuncturistData:\n",
    "    \"\"\"Data structure for acupuncturist information\"\"\"\n",
    "    full_name: str = \"\"\n",
    "    business_name: str = \"\"\n",
    "    website: str = \"\"\n",
    "    email: str = \"\"\n",
    "    phone: str = \"\"\n",
    "    location: str = \"\"\n",
    "    source_url: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def get_hash(self) -> str:\n",
    "        \"\"\"Generate unique hash for deduplication\"\"\"\n",
    "        # Normalize data for comparison\n",
    "        name = self.full_name.lower().strip()\n",
    "        business = self.business_name.lower().strip()\n",
    "        phone = re.sub(r'[^\\d]', '', self.phone)\n",
    "        email = self.email.lower().strip()\n",
    "        \n",
    "        # Create hash from key fields\n",
    "        key = f\"{name}|{business}|{phone}|{email}\"\n",
    "        return hashlib.md5(key.encode()).hexdigest()\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"Base class for all scrapers - requests only version\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Static list of user agents (no need for fake_useragent library)\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
    "        ]\n",
    "        user_agent = random.choice(user_agents)\n",
    "        \n",
    "        self.session.headers.update({\n",
    "            'User-Agent': user_agent,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5,sv;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive'\n",
    "        })\n",
    "    \n",
    "    def get_page(self, url: str, wait_time: int = None) -> BeautifulSoup:\n",
    "        \"\"\"Get page content with enhanced anti-detection measures\"\"\"\n",
    "        if wait_time is None:\n",
    "            wait_time = random.uniform(2, 5)  # Longer delays to appear human\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Add cookies and session handling\n",
    "        if not hasattr(self, '_session_initialized'):\n",
    "            # Visit homepage first to get session cookies\n",
    "            try:\n",
    "                domain = urlparse(url).netloc\n",
    "                homepage = f\"https://{domain}\"\n",
    "                self.session.get(homepage, timeout=10)\n",
    "                self._session_initialized = True\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Enhanced headers to appear more legitimate\n",
    "        headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'sv-SE,sv;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'sec-ch-ua': '\"Google Chrome\";v=\"107\", \"Chromium\";v=\"107\", \"Not=A?Brand\";v=\"24\"',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'sec-ch-ua-platform': '\"macOS\"',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15, headers=headers, allow_redirects=True)\n",
    "            \n",
    "            # Handle different status codes\n",
    "            if response.status_code == 403:\n",
    "                logger.warning(f\"403 Forbidden for {url} - trying alternative approach\")\n",
    "                return BeautifulSoup(\"\", 'html.parser')\n",
    "            elif response.status_code == 429:\n",
    "                logger.warning(f\"Rate limited for {url} - waiting longer\")\n",
    "                time.sleep(10)\n",
    "                return BeautifulSoup(\"\", 'html.parser')\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching {url}: {e}\")\n",
    "            return BeautifulSoup(\"\", 'html.parser')\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        return re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    def extract_email(self, text: str) -> str:\n",
    "        \"\"\"Extract email from text\"\"\"\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        return emails[0] if emails else \"\"\n",
    "    \n",
    "    def extract_phone(self, text: str) -> str:\n",
    "        \"\"\"Extract Swedish phone number from text\"\"\"\n",
    "        # Swedish phone patterns\n",
    "        patterns = [\n",
    "            r'\\b(?:\\+46|0)(?:\\s*-?\\s*)?(?:\\d{1,4}(?:\\s*-?\\s*)?){2,4}\\b',\n",
    "            r'\\b\\d{2,4}[-\\s]\\d{2,4}[-\\s]\\d{2,4}\\b',\n",
    "            r'\\b\\d{10,11}\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            phones = re.findall(pattern, text)\n",
    "            if phones:\n",
    "                return phones[0].strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Override this method in each scraper\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class HittaScraper(BaseScraper):\n",
    "    \"\"\"Scraper for Hitta.se (Swedish business directory)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://www.hitta.se\"\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        results = []\n",
    "        \n",
    "        # Different search terms for comprehensive coverage\n",
    "        search_terms = [\n",
    "            \"akupunktur\",\n",
    "            \"akupunktör\",\n",
    "            \"traditionell kinesisk medicin\",\n",
    "            \"TCM\"\n",
    "        ]\n",
    "        \n",
    "        for term in search_terms:\n",
    "            try:\n",
    "                search_url = f\"{self.base_url}/sök/{term}\"\n",
    "                soup = self.get_page(search_url)\n",
    "                \n",
    "                # Find business listings\n",
    "                businesses = soup.find_all(['div', 'article'], class_=re.compile(r'hit|result|listing'))\n",
    "                \n",
    "                for business in businesses:\n",
    "                    data = AcupuncturistData()\n",
    "                    data.source_url = search_url\n",
    "                    \n",
    "                    # Extract business name\n",
    "                    name_selectors = ['h2', 'h3', '.company-name', '.business-name', '.name']\n",
    "                    for selector in name_selectors:\n",
    "                        name_elem = business.find(selector)\n",
    "                        if name_elem:\n",
    "                            data.business_name = self.clean_text(name_elem.get_text())\n",
    "                            break\n",
    "                    \n",
    "                    # Extract location/address\n",
    "                    address_selectors = ['.address', '.location', '.locality']\n",
    "                    for selector in address_selectors:\n",
    "                        addr_elem = business.find(selector)\n",
    "                        if addr_elem:\n",
    "                            data.location = self.clean_text(addr_elem.get_text())\n",
    "                            break\n",
    "                    \n",
    "                    # Extract contact info from all text\n",
    "                    all_text = business.get_text()\n",
    "                    data.email = self.extract_email(all_text)\n",
    "                    data.phone = self.extract_phone(all_text)\n",
    "                    \n",
    "                    # Extract website\n",
    "                    links = business.find_all('a', href=True)\n",
    "                    for link in links:\n",
    "                        href = link['href']\n",
    "                        if any(domain in href for domain in ['.se', '.com', '.net', '.org']) and 'hitta.se' not in href:\n",
    "                            data.website = href\n",
    "                            break\n",
    "                    \n",
    "                    if data.business_name and ('akupunktur' in data.business_name.lower() or 'tcm' in data.business_name.lower()):\n",
    "                        results.append(data)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping Hitta for {term}: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "class YellowPagesScraper(BaseScraper):\n",
    "\n",
    "    class EniroScraper(BaseScraper):\n",
    "        \"\"\"Scraper for Eniro.se (Swedish business directory)\"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.base_url = \"https://www.eniro.se\"\n",
    "        \n",
    "        def scrape(self) -> List[AcupuncturistData]:\n",
    "            results = []\n",
    "            \n",
    "            search_terms = [\"akupunktur\", \"akupunktör\", \"traditionell kinesisk medicin\"]\n",
    "            \n",
    "            for term in search_terms:\n",
    "                try:\n",
    "                    search_url = f\"{self.base_url}/sok/foretagsuppgifter?q={term}\"\n",
    "                    soup = self.get_page(search_url)\n",
    "                    \n",
    "                    # Find business listings\n",
    "                    businesses = soup.find_all(['div', 'article'], class_=re.compile(r'company|hit|result'))\n",
    "                    \n",
    "                    for business in businesses:\n",
    "                        data = AcupuncturistData()\n",
    "                        data.source_url = search_url\n",
    "                        \n",
    "                        # Extract business name\n",
    "                        name_elem = business.find(['h2', 'h3', '.company-name', '.name'])\n",
    "                        if name_elem:\n",
    "                            data.business_name = self.clean_text(name_elem.get_text())\n",
    "                        \n",
    "                        # Extract location\n",
    "                        address_elem = business.find(['.address', '.location'])\n",
    "                        if address_elem:\n",
    "                            data.location = self.clean_text(address_elem.get_text())\n",
    "                        \n",
    "                        # Extract phone\n",
    "                        phone_elem = business.find(['.phone', '.telephone'])\n",
    "                        if phone_elem:\n",
    "                            data.phone = self.clean_text(phone_elem.get_text())\n",
    "                        \n",
    "                        # Extract email and website from links\n",
    "                        links = business.find_all('a', href=True)\n",
    "                        for link in links:\n",
    "                            href = link['href']\n",
    "                            if 'mailto:' in href:\n",
    "                                data.email = href.replace('mailto:', '')\n",
    "                            elif any(domain in href for domain in ['.se', '.com', '.net']) and 'eniro.se' not in href:\n",
    "                                data.website = href\n",
    "                        \n",
    "                        if data.business_name:\n",
    "                            results.append(data)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error scraping Eniro for {term}: {e}\")\n",
    "            \n",
    "            return results\n",
    "\n",
    "class RatsitScraper(BaseScraper):\n",
    "    \"\"\"Scraper for Ratsit.se\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://www.ratsit.se\"\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_url = f\"{self.base_url}/sök?q=akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            # Find business/person listings\n",
    "            listings = soup.find_all(['div', 'article'], class_=re.compile(r'result|hit|listing'))\n",
    "            \n",
    "            for listing in listings:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract name (could be person or business)\n",
    "                name_elem = listing.find(['h2', 'h3', '.name', '.title'])\n",
    "                if name_elem:\n",
    "                    name_text = self.clean_text(name_elem.get_text())\n",
    "                    # Determine if it's a person or business name\n",
    "                    if any(word in name_text.lower() for word in ['ab', 'akupunktur', 'klinik', 'center']):\n",
    "                        data.business_name = name_text\n",
    "                    else:\n",
    "                        data.full_name = name_text\n",
    "                \n",
    "                # Extract location\n",
    "                location_elem = listing.find(['.address', '.location', '.area'])\n",
    "                if location_elem:\n",
    "                    data.location = self.clean_text(location_elem.get_text())\n",
    "                \n",
    "                # Extract contact info\n",
    "                all_text = listing.get_text()\n",
    "                data.email = self.extract_email(all_text)\n",
    "                data.phone = self.extract_phone(all_text)\n",
    "                \n",
    "                if data.full_name or data.business_name:\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Ratsit: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "class HealthProfessionalsScraper(BaseScraper):\n",
    "    \"\"\"Scraper for healthcare professional directories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def scrape_bokadirekt(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Scrape BokaDirekt.se for acupuncturists\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_url = \"https://www.bokadirekt.se/boka/akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            practitioners = soup.find_all(['div', 'article'], class_=re.compile(r'practitioner|provider|therapist'))\n",
    "            \n",
    "            for practitioner in practitioners:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract practitioner name\n",
    "                name_elem = practitioner.find(['h2', 'h3', '.name', '.practitioner-name'])\n",
    "                if name_elem:\n",
    "                    data.full_name = self.clean_text(name_elem.get_text())\n",
    "                \n",
    "                # Extract clinic/business name\n",
    "                clinic_elem = practitioner.find(['.clinic', '.business', '.practice'])\n",
    "                if clinic_elem:\n",
    "                    data.business_name = self.clean_text(clinic_elem.get_text())\n",
    "                \n",
    "                # Extract location\n",
    "                location_elem = practitioner.find(['.location', '.address', '.area'])\n",
    "                if location_elem:\n",
    "                    data.location = self.clean_text(location_elem.get_text())\n",
    "                \n",
    "                # Extract contact info\n",
    "                all_text = practitioner.get_text()\n",
    "                data.email = self.extract_email(all_text)\n",
    "                data.phone = self.extract_phone(all_text)\n",
    "                \n",
    "                # Extract website\n",
    "                links = practitioner.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link['href']\n",
    "                    if any(domain in href for domain in ['.se', '.com']) and 'bokadirekt.se' not in href:\n",
    "                        data.website = href\n",
    "                        break\n",
    "                \n",
    "                if data.full_name or data.business_name:\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping BokaDirekt: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape_vardguiden(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Scrape 1177.se/Vårdguiden for acupuncturists\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            # 1177.se often has healthcare provider listings\n",
    "            search_url = \"https://www.1177.se/hitta-vard/?q=akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            providers = soup.find_all(['div', 'article'], class_=re.compile(r'provider|clinic|unit'))\n",
    "            \n",
    "            for provider in providers:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract clinic name\n",
    "                name_elem = provider.find(['h2', 'h3', '.name', '.unit-name'])\n",
    "                if name_elem:\n",
    "                    data.business_name = self.clean_text(name_elem.get_text())\n",
    "                \n",
    "                # Extract location\n",
    "                address_elem = provider.find(['.address', '.location'])\n",
    "                if address_elem:\n",
    "                    data.location = self.clean_text(address_elem.get_text())\n",
    "                \n",
    "                # Extract contact info\n",
    "                contact_elem = provider.find(['.contact', '.phone'])\n",
    "                if contact_elem:\n",
    "                    contact_text = contact_elem.get_text()\n",
    "                    data.phone = self.extract_phone(contact_text)\n",
    "                    data.email = self.extract_email(contact_text)\n",
    "                \n",
    "                if data.business_name and 'akupunktur' in data.business_name.lower():\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Vårdguiden: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Run all healthcare directory scrapers\"\"\"\n",
    "        results = []\n",
    "        results.extend(self.scrape_bokadirekt())\n",
    "        results.extend(self.scrape_vardguiden())\n",
    "        return results\n",
    "\n",
    "class GoogleSearchScraper(BaseScraper):\n",
    "    \"\"\"Scraper for Google search results - more accessible\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        results = []\n",
    "        \n",
    "        # Search terms for Swedish acupuncturists\n",
    "        search_queries = [\n",
    "            \"akupunktur stockholm site:se\",\n",
    "            \"akupunktör göteborg site:se\", \n",
    "            \"traditionell kinesisk medicin malmö site:se\",\n",
    "            \"akupunktur uppsala site:se\"\n",
    "        ]\n",
    "        \n",
    "        for query in search_queries:\n",
    "            try:\n",
    "                # Use Google search (be careful not to abuse)\n",
    "                search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}&num=20\"\n",
    "                soup = self.get_page(search_url, wait_time=random.uniform(3, 6))\n",
    "                \n",
    "                # Find search result links\n",
    "                results_divs = soup.find_all('div', class_='g')\n",
    "                \n",
    "                for result_div in results_divs:\n",
    "                    try:\n",
    "                        # Extract link\n",
    "                        link_elem = result_div.find('a', href=True)\n",
    "                        if not link_elem:\n",
    "                            continue\n",
    "                            \n",
    "                        website = link_elem['href']\n",
    "                        if not website.startswith('http'):\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract title (potential business name)\n",
    "                        title_elem = result_div.find('h3')\n",
    "                        if not title_elem:\n",
    "                            continue\n",
    "                            \n",
    "                        title = self.clean_text(title_elem.get_text())\n",
    "                        \n",
    "                        # Extract snippet for contact info\n",
    "                        snippet_elem = result_div.find(['span', 'div'], class_=re.compile(r'st|s3v9rd'))\n",
    "                        snippet_text = snippet_elem.get_text() if snippet_elem else \"\"\n",
    "                        \n",
    "                        data = AcupuncturistData()\n",
    "                        data.source_url = search_url\n",
    "                        data.business_name = title\n",
    "                        data.website = website\n",
    "                        data.email = self.extract_email(snippet_text)\n",
    "                        data.phone = self.extract_phone(snippet_text)\n",
    "                        \n",
    "                        # Try to extract location from snippet\n",
    "                        if any(city in snippet_text.lower() for city in ['stockholm', 'göteborg', 'malmö', 'uppsala']):\n",
    "                            data.location = snippet_text[:100]  # First part likely contains location\n",
    "                        \n",
    "                        if data.business_name and 'akupunktur' in data.business_name.lower():\n",
    "                            results.append(data)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error processing Google result: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping Google for {query}: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "class PublicDirectoryScraper(BaseScraper):\n",
    "    \"\"\"Scraper for more open/public directories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def scrape_merinfo(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Scrape Merinfo.se - often more accessible\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_url = \"https://www.merinfo.se/search?q=akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            companies = soup.find_all(['div', 'article'], class_=re.compile(r'company|result'))\n",
    "            \n",
    "            for company in companies:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract company name\n",
    "                name_elem = company.find(['h2', 'h3', '.company-name'])\n",
    "                if name_elem:\n",
    "                    data.business_name = self.clean_text(name_elem.get_text())\n",
    "                \n",
    "                # Extract location\n",
    "                location_elem = company.find(['.address', '.location'])\n",
    "                if location_elem:\n",
    "                    data.location = self.clean_text(location_elem.get_text())\n",
    "                \n",
    "                # Extract all text for contact info\n",
    "                all_text = company.get_text()\n",
    "                data.email = self.extract_email(all_text)\n",
    "                data.phone = self.extract_phone(all_text)\n",
    "                \n",
    "                if data.business_name:\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Merinfo: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape_allabolag(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Scrape Allabolag.se for company information\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_url = \"https://www.allabolag.se/what/akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            companies = soup.find_all(['div'], class_=re.compile(r'company|hit'))\n",
    "            \n",
    "            for company in companies:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract company name\n",
    "                name_elem = company.find(['a', 'h3'], class_=re.compile(r'company|name'))\n",
    "                if name_elem:\n",
    "                    data.business_name = self.clean_text(name_elem.get_text())\n",
    "                \n",
    "                # Extract location from any address elements\n",
    "                address_text = company.get_text()\n",
    "                # Look for Swedish postal codes and cities\n",
    "                postal_match = re.search(r'\\d{3}\\s?\\d{2}\\s+([A-ZÅÄÖ][a-zåäö]+)', address_text)\n",
    "                if postal_match:\n",
    "                    data.location = postal_match.group(0)\n",
    "                \n",
    "                if data.business_name and 'akupunktur' in data.business_name.lower():\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Allabolag: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Run all public directory scrapers\"\"\"\n",
    "        results = []\n",
    "        results.extend(self.scrape_merinfo())\n",
    "        results.extend(self.scrape_allabolag())\n",
    "        return results\n",
    "    \"\"\"Scraper for GulaS.se (Swedish Yellow Pages)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_url = \"https://www.gulasidorna.se\"\n",
    "    \n",
    "    def scrape(self) -> List[AcupuncturistData]:\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            search_url = f\"{self.base_url}/sok?q=akupunktur\"\n",
    "            soup = self.get_page(search_url)\n",
    "            \n",
    "            # Find business listings\n",
    "            businesses = soup.find_all(['div', 'article'], class_=re.compile(r'company|business|listing'))\n",
    "            \n",
    "            for business in businesses:\n",
    "                data = AcupuncturistData()\n",
    "                data.source_url = search_url\n",
    "                \n",
    "                # Extract business name\n",
    "                name_elem = business.find(['h2', 'h3', '.company-name'])\n",
    "                if name_elem:\n",
    "                    data.business_name = self.clean_text(name_elem.get_text())\n",
    "                \n",
    "                # Extract location\n",
    "                address_elem = business.find(['.address', '.location'])\n",
    "                if address_elem:\n",
    "                    data.location = self.clean_text(address_elem.get_text())\n",
    "                \n",
    "                # Extract contact details\n",
    "                all_text = business.get_text()\n",
    "                data.email = self.extract_email(all_text)\n",
    "                data.phone = self.extract_phone(all_text)\n",
    "                \n",
    "                # Extract website\n",
    "                links = business.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link['href']\n",
    "                    if 'http' in href and 'gulasidorna.se' not in href:\n",
    "                        data.website = href\n",
    "                        break\n",
    "                \n",
    "                if data.business_name:\n",
    "                    results.append(data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Yellow Pages: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "class AcupuncturistDataCollector:\n",
    "    \"\"\"Main class to coordinate all scrapers and manage data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scrapers = [\n",
    "            PublicDirectoryScraper(),  # More accessible\n",
    "            GoogleSearchScraper(),     # Alternative approach\n",
    "            HittaScraper(),\n",
    "            # EniroScraper(),          # Often blocked, disabled for now\n",
    "            RatsitScraper(),\n",
    "            HealthProfessionalsScraper(),\n",
    "            YellowPagesScraper(),\n",
    "        ]\n",
    "        self.all_data: List[AcupuncturistData] = []\n",
    "        self.unique_hashes: Set[str] = set()\n",
    "    \n",
    "    def run_all_scrapers(self):\n",
    "        \"\"\"Run all scrapers and collect data\"\"\"\n",
    "        logger.info(\"Starting data collection from all sources...\")\n",
    "        \n",
    "        for scraper in self.scrapers:\n",
    "            scraper_name = scraper.__class__.__name__\n",
    "            logger.info(f\"Running {scraper_name}...\")\n",
    "            \n",
    "            try:\n",
    "                data = scraper.scrape()\n",
    "                logger.info(f\"{scraper_name} collected {len(data)} entries\")\n",
    "                self.all_data.extend(data)\n",
    "                \n",
    "                # Add delay between scrapers\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {scraper_name}: {e}\")\n",
    "    \n",
    "    def deduplicate_data(self) -> List[AcupuncturistData]:\n",
    "        \"\"\"Remove duplicate entries based on hash\"\"\"\n",
    "        unique_data = []\n",
    "        \n",
    "        for entry in self.all_data:\n",
    "            entry_hash = entry.get_hash()\n",
    "            if entry_hash not in self.unique_hashes:\n",
    "                self.unique_hashes.add(entry_hash)\n",
    "                unique_data.append(entry)\n",
    "        \n",
    "        logger.info(f\"Removed {len(self.all_data) - len(unique_data)} duplicates\")\n",
    "        return unique_data\n",
    "    \n",
    "    def clean_and_validate_data(self, data: List[AcupuncturistData]) -> List[AcupuncturistData]:\n",
    "        \"\"\"Clean and validate collected data\"\"\"\n",
    "        cleaned_data = []\n",
    "        \n",
    "        for entry in data:\n",
    "            # Skip entries without essential information\n",
    "            if not entry.full_name and not entry.business_name:\n",
    "                continue\n",
    "            \n",
    "            # Clean phone numbers\n",
    "            if entry.phone:\n",
    "                entry.phone = re.sub(r'[^\\d\\+\\-\\s]', '', entry.phone)\n",
    "                # Ensure Swedish phone format\n",
    "                if entry.phone and not entry.phone.startswith(('+46', '0')):\n",
    "                    entry.phone = f\"0{entry.phone}\" if len(entry.phone) >= 9 else entry.phone\n",
    "            \n",
    "            # Validate email\n",
    "            if entry.email and '@' not in entry.email:\n",
    "                entry.email = \"\"\n",
    "            \n",
    "            # Clean website URLs\n",
    "            if entry.website:\n",
    "                if not entry.website.startswith('http'):\n",
    "                    entry.website = f\"https://{entry.website}\"\n",
    "                # Remove tracking parameters\n",
    "                entry.website = re.sub(r'\\?.*$', '', entry.website)\n",
    "            \n",
    "            # Clean location\n",
    "            if entry.location:\n",
    "                entry.location = re.sub(r'\\s+', ' ', entry.location)\n",
    "            \n",
    "            cleaned_data.append(entry)\n",
    "        \n",
    "        return cleaned_data\n",
    "    \n",
    "    def export_to_excel(self, filename: str = \"acupuncturists_sweden.xlsx\"):\n",
    "        \"\"\"Export data to Excel file\"\"\"\n",
    "        unique_data = self.deduplicate_data()\n",
    "        cleaned_data = self.clean_and_validate_data(unique_data)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([entry.to_dict() for entry in cleaned_data])\n",
    "        \n",
    "        # Reorder columns\n",
    "        column_order = ['full_name', 'business_name', 'website', 'email', 'phone', 'location', 'source_url']\n",
    "        df = df.reindex(columns=column_order)\n",
    "        \n",
    "        # Export to Excel with formatting\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name='Acupuncturists')\n",
    "            \n",
    "            # Auto-adjust column widths\n",
    "            worksheet = writer.sheets['Acupuncturists']\n",
    "            for column in worksheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                for cell in column:\n",
    "                    try:\n",
    "                        if len(str(cell.value)) > max_length:\n",
    "                            max_length = len(str(cell.value))\n",
    "                    except:\n",
    "                        pass\n",
    "                adjusted_width = min(max_length + 2, 50)\n",
    "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        logger.info(f\"Exported {len(df)} unique entries to {filename}\")\n",
    "        return df\n",
    "    \n",
    "    def export_to_csv(self, filename: str = \"acupuncturists_sweden.csv\"):\n",
    "        \"\"\"Export data to CSV file\"\"\"\n",
    "        unique_data = self.deduplicate_data()\n",
    "        cleaned_data = self.clean_and_validate_data(unique_data)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([entry.to_dict() for entry in cleaned_data])\n",
    "        \n",
    "        # Reorder columns\n",
    "        column_order = ['full_name', 'business_name', 'website', 'email', 'phone', 'location', 'source_url']\n",
    "        df = df.reindex(columns=column_order)\n",
    "        \n",
    "        # Export to CSV\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Exported {len(df)} unique entries to {filename}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get collection statistics\"\"\"\n",
    "        unique_data = self.deduplicate_data()\n",
    "        \n",
    "        stats = {\n",
    "            'total_collected': len(self.all_data),\n",
    "            'unique_entries': len(unique_data),\n",
    "            'duplicates_removed': len(self.all_data) - len(unique_data),\n",
    "            'entries_with_email': len([d for d in unique_data if d.email]),\n",
    "            'entries_with_phone': len([d for d in unique_data if d.phone]),\n",
    "            'entries_with_website': len([d for d in unique_data if d.website]),\n",
    "            'business_names': len([d for d in unique_data if d.business_name]),\n",
    "            'full_names': len([d for d in unique_data if d.full_name])\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Simple usage example\n",
    "def quick_scrape():\n",
    "    \"\"\"Quick scraping function for testing\"\"\"\n",
    "    collector = AcupuncturistDataCollector()\n",
    "    \n",
    "    # Run just one scraper for testing (use more accessible one)\n",
    "    public_scraper = PublicDirectoryScraper()\n",
    "    data = public_scraper.scrape()\n",
    "    collector.all_data = data\n",
    "    \n",
    "    # If no data, try Google search as backup\n",
    "    if len(data) == 0:\n",
    "        logger.info(\"No data from public directories, trying Google search...\")\n",
    "        google_scraper = GoogleSearchScraper()\n",
    "        backup_data = google_scraper.scrape()\n",
    "        collector.all_data = backup_data\n",
    "    \n",
    "    # Export results\n",
    "    df = collector.export_to_csv(\"test_acupuncturists.csv\")\n",
    "    stats = collector.get_statistics()\n",
    "    \n",
    "    print(\"Quick scrape results:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Swedish Acupuncturist Data Scraper\")\n",
    "    print(\"==================================\")\n",
    "    \n",
    "    # Ask user for preference\n",
    "    choice = input(\"Run full scraping (f) or quick test (q)? [f/q]: \").lower()\n",
    "    \n",
    "    if choice == 'q':\n",
    "        df = quick_scrape()\n",
    "    else:\n",
    "        # Create collector and run all scrapers\n",
    "        collector = AcupuncturistDataCollector()\n",
    "        \n",
    "        # Run collection\n",
    "        collector.run_all_scrapers()\n",
    "        \n",
    "        # Export results\n",
    "        df = collector.export_to_excel()\n",
    "        collector.export_to_csv()\n",
    "        \n",
    "        # Show statistics\n",
    "        stats = collector.get_statistics()\n",
    "        \n",
    "        print(f\"\\nCollection Summary:\")\n",
    "        print(f\"==================\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        if len(df) > 0:\n",
    "            print(\"\\nSample data (first 3 rows):\")\n",
    "            print(\"===========================\")\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            print(df.head(3).to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nNo data collected. Please check the scrapers and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
